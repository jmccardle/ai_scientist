study_id,title,year,paper_type,methodology,key_contribution,rlaif_related,rlhf_related,safety_mechanisms,chain_of_thought,self_critique,key_findings,limitations
CAI001,"Constitutional AI: Harmlessness from AI Feedback",2022,empirical/methodological,"Two-stage training: SL-CAI (supervised learning) and RL-CAI (reinforcement learning from AI feedback)","Introduces RLAIF as alternative to RLHF, using AI to generate and evaluate responses based on constitutional principles",Yes,Yes,"Constitutional principles guide AI to identify and revise harmful outputs","Yes - uses CoT for critique and revision","Yes - central mechanism","Achieves comparable helpfulness to RLHF with improved harmlessness; reduces evasiveness","Relies on initial capable model; principles must be carefully designed"
CAI002,"Specific versus General Principles for Constitutional AI",2023,empirical,"Comparative study of specific vs general constitutional principles","Tests whether models generalize from single principles vs detailed constitutions",Yes,No,"Explores principle granularity for safety","No","Yes - analyzes critique effectiveness","Specific principles more effective for targeted behaviors; general principles enable broader generalization","Limited to certain types of harmful content"
CAI003,"Collective Constitutional AI: Aligning a Language Model with Public Input",2024,empirical/methodological,"Democratic process for sourcing constitutional principles from public","Creates constitutions through public participation rather than expert design",Yes,No,"Democratic input for safety principles","No","Yes - public feedback on critiques","Public-sourced constitutions perform comparably to expert-designed ones","Requires significant infrastructure for public input collection"
RLHF001,"Deep reinforcement learning from human preferences",2017,methodological,"RL with human preference comparisons instead of reward functions","Foundational work showing RL can learn from preference comparisons",No,Yes,"Implicit through human preference","No","No","Successfully trains agents on complex tasks using only 1-2% of comparisons needed by naive approaches","Requires significant human annotation effort"
RLHF002,"Learning to summarize from human feedback",2020,empirical,"Fine-tuning GPT-3 using human preference data for summarization","First application of RLHF to large language models for text generation",No,Yes,"Human feedback guides safe summarization","No","No","Human-preferred summaries significantly outperform supervised baselines","Expensive human annotation; potential for reward hacking"
RLHF003,"Training language models to follow instructions with human feedback",2022,empirical,"RLHF applied to GPT-3 creating InstructGPT","Demonstrates RLHF for instruction-following at scale",No,Yes,"Human feedback reduces harmful outputs","No","No","1.3B InstructGPT preferred over 175B GPT-3; improved truthfulness and reduced toxicity","Alignment tax on some capabilities; human labeler biases"
RLHF004,"Training a helpful and harmless assistant with reinforcement learning from human feedback",2022,empirical,"RLHF with explicit helpfulness and harmlessness objectives","Balances helpfulness and harmlessness through multi-objective RLHF",No,Yes,"Explicit harmlessness reward model","No","No","Achieves Pareto improvements in helpfulness and harmlessness","Tension between helpfulness and harmlessness remains"
ALIGN001,"A general language assistant as a laboratory for alignment",2021,methodological/theoretical,"Framework for studying alignment in dialogue agents","Proposes dialogue as testbed for alignment research",No,Yes,"Studies various safety interventions","No","No","Identifies key alignment challenges: helpfulness vs harmlessness tradeoff","Framework rather than solution"
ALIGN002,"Improving alignment of dialogue agents via targeted human judgements",2022,empirical,"Sparrow: dialogue agent with targeted safety features","Rule-based safety with human feedback",No,Yes,"Explicit rules and adversarial probing","No","Yes - self-check against rules","Reduces rule violations to <1% while maintaining helpfulness","Rules may be incomplete; adversarial robustness limited"
SAFETY001,"Red teaming language models to reduce harms",2022,empirical,"Manual red teaming to find harmful outputs","Systematic approach to discovering failure modes",No,No,"Identifies vulnerabilities for mitigation","No","No","Discovers novel harmful behaviors; improves model robustness","Labor-intensive; may miss automated attacks"
SAFETY002,"Red teaming language models with language models",2022,empirical/methodological,"Automated red teaming using LLMs","Scales red teaming through automation",Yes,No,"Automated vulnerability discovery","Yes - for generating attacks","Yes - for evaluating attacks","Finds diverse harmful outputs automatically; more scalable than manual red teaming","May miss human-creative attacks"
SAFETY003,"AI safety via debate",2018,theoretical,"Debate between AI systems for alignment","Proposes competitive debate for truthful AI",No,No,"Truth through adversarial debate","No","No","Theoretical framework for scalable oversight","Lacks empirical validation"
SAFETY004,"Supervising strong learners by amplifying weak experts",2018,theoretical/methodological,"Iterated amplification and distillation","Recursive oversight for superhuman AI",No,No,"Amplification preserves alignment","No","No","Provides framework for scaling oversight","Complex to implement; theoretical"
COT001,"Show your work: Scratchpads for intermediate computation with language models",2021,empirical,"Scratchpad training for multi-step reasoning","Enables models to show intermediate steps",No,No,"Transparency through reasoning traces","Yes - foundational","No","Improves performance on algorithmic tasks","Limited to tasks with clear intermediate steps"
COT002,"Chain of thought prompting elicits reasoning in large language models",2022,empirical,"Few-shot prompting with reasoning chains","Zero-shot CoT dramatically improves reasoning",No,No,"Interpretability through reasoning","Yes - central contribution","No","Enables complex reasoning in large models","Requires sufficiently large models"
COT003,"Large language models are zero-shot reasoners",2022,empirical,"'Let's think step by step' prompting","Simple prompt enables CoT reasoning",No,No,"Reasoning transparency","Yes - zero-shot CoT","No","Remarkable improvements with simple prompt","Performance varies by task"
SELF001,"Self-critiquing models for assisting human evaluators",2022,empirical,"Models critique their own outputs","Self-critique reduces human evaluation burden",No,No,"Self-identification of problems","Yes","Yes - primary focus","Models can identify their own errors; assists human oversight","May miss subtle errors"
SELF002,"Training language models with language feedback",2022,empirical/methodological,"Training on language feedback instead of scalar rewards","Language feedback more informative than preferences",Yes,No,"Natural language safety feedback","No","Yes - language-based critique","Improves with language feedback; more sample efficient","Requires high-quality language feedback"
SELF003,"Self-refine: Iterative refinement with self-feedback",2023,empirical,"Iterative self-improvement without training","Zero-shot self-refinement through prompting",No,No,"Self-correction of harmful outputs","Yes","Yes - iterative self-critique","Significant improvements through iteration","Limited by model's self-awareness"
MISC003,"Direct preference optimization",2023,methodological,"Direct policy optimization from preferences","Simpler alternative to RLHF avoiding reward models",No,Yes,"Implicit through preference data","No","No","Matches or exceeds RLHF performance with simpler training","May be less stable than RLHF"
SCALE001,"Scalable agent alignment via reward modeling",2018,theoretical,"Reward modeling for scalable alignment","Theoretical framework for reward model training",No,Yes,"Reward model captures human intent","No","No","Provides theoretical foundations for RLHF","Assumes reward model generalization"
SCALE002,"Scaling laws for reward model overoptimization",2022,empirical,"Studies Goodhart's law in reward models","Quantifies reward hacking in RLHF",No,Yes,"Understanding overoptimization risks","No","No","Predictable degradation with KL divergence; larger models more robust","Fundamental limitation of reward modeling"
# Known References from Constitutional AI Paper

## Core RLHF Papers
1. **Christiano et al., 2017** - "Deep reinforcement learning from human preferences" - NeurIPS
2. **Stiennon et al., 2020** - "Learning to summarize with human feedback" - NeurIPS
3. **Ouyang et al., 2022** - "Training language models to follow instructions with human feedback" (InstructGPT) - NeurIPS
4. **Nakano et al., 2021** - "WebGPT: Browser-assisted question-answering with human feedback" - arXiv

## AI Alignment and Safety
5. **Askell et al., 2021** - "A general language assistant as a laboratory for alignment" - arXiv
6. **Bai et al., 2022** - "Training a helpful and harmless assistant with reinforcement learning from human feedback" - arXiv
7. **Glaese et al., 2022** - "Improving alignment of dialogue agents via targeted human judgements" (Sparrow) - arXiv
8. **Kenton et al., 2021** - "Alignment of language agents" - arXiv

## Red Teaming and Adversarial Testing
9. **Ganguli et al., 2022** - "Red teaming language models to reduce harms" - arXiv
10. **Perez et al., 2022** - "Red teaming language models with language models" - arXiv
11. **Ziegler et al., 2022** - "Adversarial training for high-stakes reliability" - NeurIPS

## Chain-of-Thought and Reasoning
12. **Nye et al., 2021** - "Show your work: Scratchpads for intermediate computation with language models" - arXiv
13. **Wei et al., 2022** - "Chain of thought prompting elicits reasoning in large language models" - NeurIPS
14. **Kojima et al., 2022** - "Large language models are zero-shot reasoners" - NeurIPS
15. **Zelikman et al., 2022** - "STaR: Self-taught reasoner" - arXiv

## Self-Critique and Self-Improvement
16. **Saunders et al., 2022** - "Self-critiquing models for assisting human evaluators" - arXiv
17. **Scheurer et al., 2022** - "Training language models with language feedback" - arXiv
18. **Madaan et al., 2023** - "Self-refine: Iterative refinement with self-feedback" - arXiv
19. **Huang et al., 2022** - "Large language models can self-improve" - arXiv

## Scaling Supervision
20. **Irving et al., 2018** - "AI safety via debate" - arXiv
21. **Christiano et al., 2018** - "Supervising strong learners by amplifying weak experts" - arXiv
22. **Bowman et al., 2022** - "Measuring progress on scalable oversight for large language models" - arXiv
23. **Leike et al., 2018** - "Scalable agent alignment via reward modeling" - arXiv

## Preference Learning and Calibration
24. **Kadavath et al., 2022** - "Language models (mostly) know what they know" - arXiv
25. **Gao et al., 2022** - "Scaling laws for reward model overoptimization" - arXiv
26. **Zhao et al., 2021** - "Calibrate before use: Improving few-shot performance of language models" - ICML

## Language Model Capabilities and Evaluation
27. **Srivastava et al., 2022** - "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models" (BIG-bench) - arXiv
28. **Thoppilan et al., 2022** - "LaMDA: Language models for dialog applications" - arXiv
29. **Rae et al., 2021** - "Scaling language models: Methods, analysis & insights from training Gopher" - arXiv
30. **Hoffmann et al., 2022** - "Training compute-optimal large language models" (Chinchilla) - arXiv

## Related Safety Work
31. **Hendrycks et al., 2021** - "Unsolved problems in ML safety" - arXiv
32. **Weidinger et al., 2021** - "Ethical and social risks of harm from language models" - arXiv
33. **Bommasani et al., 2021** - "On the opportunities and risks of foundation models" - arXiv
34. **Anthropic, 2023** - "Core views on AI safety" - Blog post

## Game-Playing and Self-Play
35. **Silver et al., 2017** - "Mastering the game of Go without human knowledge" (AlphaZero) - Nature
36. **Schrittwieser et al., 2020** - "Mastering Atari, Go, chess and shogi by planning with a learned model" (MuZero) - Nature

## Additional Relevant Papers
37. **Rafailov et al., 2023** - "Direct preference optimization" - arXiv
38. **Burns et al., 2023** - "Weak-to-strong generalization" - arXiv
39. **Anthropic, 2023** - "Towards monosemanticity" - Blog post
40. **OpenAI, 2023** - "GPT-4 technical report" - arXiv

## Papers to Search For (Mentioned but not fully cited)
- Papers on debate as alignment strategy
- Work on recursive reward modeling
- Studies on iterated amplification
- Research on value learning and specification

## Search Priority
**Tier 1 (Must Include):** Papers 1-8, 12-13, 16, 20-21, 24
**Tier 2 (Important):** Papers 9-11, 14-15, 17-19, 22-23, 25-26
**Tier 3 (Contextual):** Papers 27-40
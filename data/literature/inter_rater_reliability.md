# Inter-Rater Reliability Assessment
## Title/Abstract Screening Phase

### Cohen's Kappa Calculation

**Total Papers Screened:** 75

**Agreement Matrix:**
|              | Reviewer 2: Include | Reviewer 2: Exclude |
|--------------|-------------------|-------------------|
| **Reviewer 1: Include** | 69 | 1 |
| **Reviewer 1: Exclude** | 0 | 5 |

**Observed Agreement (Po):** 74/75 = 0.987

**Expected Agreement by Chance (Pe):**
- P(R1 Include) = 70/75 = 0.933
- P(R2 Include) = 69/75 = 0.920
- P(R1 Exclude) = 5/75 = 0.067
- P(R2 Exclude) = 6/75 = 0.080
- Pe = (0.933 × 0.920) + (0.067 × 0.080) = 0.859 + 0.005 = 0.864

**Cohen's Kappa (κ):**
κ = (Po - Pe) / (1 - Pe)
κ = (0.987 - 0.864) / (1 - 0.864)
κ = 0.123 / 0.136
**κ = 0.904**

### Interpretation
- κ = 0.904 indicates **almost perfect agreement** (>0.81)
- Well above the required threshold of κ > 0.6
- Only 1 disagreement resolved through third reviewer assessment

### Disagreement Resolution
**Study 038:** "AI Safety via Market Making"
- Reviewer 1: Include (novel economic approach to alignment)
- Reviewer 2: Exclude (too theoretical, lacks specific risk analysis)
- **Third Reviewer Decision:** Include
- **Rationale:** While theoretical, provides concrete risk mitigation framework relevant to alignment challenges

### Quality Assessment
✅ Inter-rater reliability: κ = 0.904 (almost perfect agreement)
✅ Systematic disagreement resolution process
✅ Transparent documentation of decisions
✅ Exceeds PRISMA 2020 reliability standards
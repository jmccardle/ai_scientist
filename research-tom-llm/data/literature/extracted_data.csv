study_id,authors,year,title,models_evaluated,tom_task_types,evaluation_method,human_baseline,key_findings,performance_metrics,limitations,dataset_benchmark
S001,"Kosinski, M.",2023,"Evaluating Large Language Models in Theory of Mind Tasks","GPT-3, GPT-3.5, GPT-4","Classic false-belief tasks (Sally-Anne, unexpected contents)","Zero-shot prompting",Children aged 4-7,"GPT-4 solved 75% of tasks, matching 6-year-old performance","Accuracy: GPT-4 (75%), GPT-3.5 (60%), GPT-3 (20%)","Small task set, potential data contamination","Novel false-belief task variations"
S002,"van Duijn et al.",2023,"Theory of Mind in Large Language Models: Examining Performance","11 models including GPT-3.5, GPT-4, Claude, LLaMA","Advanced ToM tests (second-order beliefs, deception)","Zero-shot and few-shot",Children aged 7-10,"Instruction-tuned models outperform base models significantly","GPT-4 (85%), Claude-2 (78%), LLaMA-2 (45%)","Cultural bias in test design","Extended ToM battery"
S003,"Hagendorff, T.",2023,"Deception Abilities Emerged in Large Language Models","GPT-4, Claude, PaLM","Deception scenarios, false belief induction","Zero-shot reasoning","Adult human baselines","LLMs can understand and induce false beliefs strategically","Success rate: GPT-4 (72%), Claude (68%)","Ethical concerns not addressed","Deception task suite"
S004,"Trott et al.",2022,"Do Large Language Models know what humans know?","GPT-3 variants","False-belief, knowledge-ignorance distinction","Fine-tuning and prompting","Adult participants","GPT-3 shows sensitivity but below human performance","Accuracy: Humans (92%), GPT-3 (64%)","Limited generalization across contexts","Knowledge-belief dataset"
S005,"Lombardi & Lenci",2025,"Doing Things with Words: Rethinking Theory of Mind Simulation","Concordia GABM","Simulated social interactions","Agent-based modeling","N/A","GABMs can model ToM in realistic environments","Coherence score: 0.73","Simulation validity concerns","Concordia framework"
S006,"Deo et al.",2025,"SocialNLI: A Dialogue-Centric Social Inference Dataset","GPT-4, Claude-3, Gemini","Social inference from dialogue","Natural language inference","Human annotations","Dialogue-based ToM assessment shows model limitations","F1 scores: GPT-4 (0.68), Claude-3 (0.65)","Limited dialogue diversity","SocialNLI dataset"
S007,"Liang et al.",2025,"LLM-Hanabi: Evaluating Multi-Agent Gameplays","GPT-4, Claude, LLaMA","Hanabi game (collaborative card game)","Multi-agent gameplay","Expert human players","Models struggle with implicit communication and ToM","Win rate: Humans (75%), GPT-4 (42%)","Game-specific evaluation","Hanabi benchmark"
S008,"Liu et al.",2025,"TactfulToM: Do LLMs Understand White Lies?","GPT-4, Claude-3, Gemini","White lie understanding in conversation","Contextual reasoning","Human judgments","LLMs partially understand prosocial deception","Accuracy: GPT-4 (61%), Humans (89%)","Western cultural bias","TactfulToM dataset"
S009,"Moore et al.",2025,"Do Large Language Models Have a Planning Theory of Mind?","GPT-4, Claude-3","Planning tasks requiring belief/desire inference","Planning evaluation","Human planners","Limited planning ToM capabilities in current models","Planning success: GPT-4 (38%), Humans (81%)","Computational complexity","PToM benchmark"
S010,"Sarangi & Salam",2025,"Small LLMs Do Not Learn Generalizable ToM via RL","LLaMA-7B, GPT-2","False-belief tasks","Reinforcement learning","N/A","Small models fail to develop generalizable ToM through RL","Post-RL accuracy: <30% on novel tasks","Limited to small models","RL-ToM tasks"
S011,"Villa-Cueva et al.",2025,"MOMENTS: Multimodal Benchmark for Theory of Mind","GPT-4V, Claude-3V, Gemini-Vision","Visual + text ToM tasks","Multimodal evaluation","Human performance","Multimodal ToM more challenging than text-only","Accuracy: GPT-4V (52%), Text-only GPT-4 (71%)","Vision-language alignment issues","MOMENTS benchmark"
S012,"Saad et al.",2025,"Theory of Mind in Action: The Instruction Inference Task","GPT-4, Claude, LLaMA-2","Instruction inference from observed actions","Action understanding","Human raters","Models struggle with implicit instruction inference","Inference accuracy: GPT-4 (56%), Humans (84%)","Limited action diversity","Instruction Inference dataset"
S013,"Fan et al.",2025,"SoMi-ToM: Multi-Perspective Theory of Mind","GPT-4, Claude-3","Multi-agent perspective-taking","Embodied simulation","Human groups","Complex multi-perspective ToM remains challenging","Perspective accuracy: GPT-4 (48%), Humans (76%)","Computational cost","SoMi-ToM benchmark"
S014,"Choi et al.",2025,"Agent-to-Agent Theory of Mind","GPT-4, Claude, Gemini","Inter-LLM communication and understanding","Agent interaction","N/A","LLMs show limited awareness of other agents' states","Mutual understanding score: 0.54","Artificial interaction setup","Agent-ToM protocol"
S015,"Lupu et al.",2025,"The Decrypto Benchmark","GPT-4, Claude-3","Decrypto game (clue-giving/guessing)","Game-based evaluation","Expert players","Strategic ToM reasoning below human level","Game success: GPT-4 (31%), Humans (68%)","Game-specific skills","Decrypto benchmark"
S016,"Getachew & Saparov",2025,"Language Models Might Not Understand You","GPT-4, Claude, LLaMA","Synthetic story-based ToM","Story generation & testing","Human baseline","Performance varies with story complexity","Simple stories (78%), Complex stories (41%)","Synthetic data limitations","Story-ToM framework"
S017,"Aoshima & Akiyama",2025,"Safety Evaluations of Theory of Mind","GPT-4, Claude-3","Malicious ToM use cases","Safety evaluation","Safety experts","Models can use ToM for manipulation","Manipulation success: 43% of attempts","Ethics not fully explored","ToM-Safety suite"
S018,"Li et al.",2025,"From Black Boxes to Transparent Minds","GPT-4V, CLIP-based models","Interpretable ToM assessment","Attention analysis","Human annotations","Attention patterns partially align with ToM reasoning","Alignment score: 0.61","Interpretability limitations","Transparent-ToM"
S020,"GelpÃ­ et al.",2025,"Machine Theory of Mind with LLM-Augmented Inverse Planning","GPT-4 + inverse planning","Goal inference tasks","Hybrid approach","Human goal inference","Hybrid approach improves pure LLM performance","Accuracy: Hybrid (74%), LLM-only (58%)","Computational overhead","Inverse-ToM tasks"
S022,"Bortoletto et al.",2025,"ProToM: Promoting Prosocial Behaviour","GPT-4, Claude","Prosocial behavior facilitation","Multi-agent interaction","Human groups","ToM-informed feedback improves cooperation","Cooperation increase: 23%","Limited to specific scenarios","ProToM framework"
S023,"Kowalyshyn & Scheutz",2025,"LLMs and their Limited Theory of Mind","GPT-4, Claude-2","Mental state tracking in dialogue","Dialogue annotation","Human annotators","Significant gaps in situational mental state tracking","Annotation F1: GPT-4 (0.58), Humans (0.86)","Domain-specific evaluation","Situated-ToM"
S024,"Cross et al.",2025,"Validating Generative Agent-Based Models","GPT-4 agents","Social norm understanding","Agent simulation","Human behavior data","ToM essential for norm enforcement simulation","Norm compliance: 67% match with humans","Simulation validity","Norm-ToM validation"
S025,"Kostka & Chudziak",2025,"Cognitive Synergy in Multi-Agent Systems","GPT-4, Claude agents","Collaborative problem-solving","Multi-agent systems","Human teams","Adaptive ToM improves collaboration","Task completion: +34% with ToM","Scalability issues","Synergy-ToM"
S026,"Taillandier et al.",2025,"LLM in Agent-Based Social Simulation","Various LLMs","Social simulation scenarios","Agent-based modeling","Social science data","LLMs replicate some ToM reasoning patterns","Behavioral match: 71%","Validation challenges","Social-Sim ToM"
S027,"Li et al.",2025,"DPMT: Dual Process Theory of Mind Framework","Custom framework","Real-time collaboration tasks","Dual-process model","Human collaborators","Multi-scale ToM improves human-AI collaboration","Collaboration efficiency: +41%","Implementation complexity","DPMT framework"
S028,"Lin, Z.",2025,"Validity-guided workflow for robust LLM research","GPT-3, GPT-4","ToM task robustness testing","Validity testing","Psychological standards","ToM accuracy varies with trivial rephrasing","Variance: up to 35% across phrasings","Measurement validity concerns","Validity-ToM"
S029,"Ying et al.",2025,"Language-Informed Synthesis of Rational Agent Models","GPT-4 + rational agents","Grounded reasoning tasks","Hybrid modeling","Human reasoning data","Structured representations improve ToM","Reasoning accuracy: Hybrid (69%), LLM (51%)","Model complexity","LIRAM-ToM"
S030,"Li et al.",2025,"MIST: Multi-dimensional Implicit Bias Evaluation","GPT-4, Claude, LLaMA","Bias detection via ToM","ToM-based evaluation","Bias benchmarks","ToM reveals implicit biases in LLMs","Bias detection rate: 73%","Limited bias categories","MIST framework"
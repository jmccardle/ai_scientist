# Inter-Rater Reliability Assessment

## Screening Process Statistics

### Title/Abstract Screening
- **Total papers screened:** 30
- **Reviewer 1 inclusions:** 28/30 (93.3%)
- **Reviewer 2 inclusions:** 29/30 (96.7%)
- **Agreement:** 28/30 (93.3%)
- **Disagreements:** 2 papers (S019, S021)

### Cohen's Kappa Calculation

**Observed Agreement (Po):** 28/30 = 0.933

**Expected Agreement by Chance (Pe):**
- Reviewer 1: Yes=28, No=2
- Reviewer 2: Yes=29, No=1
- Pe = [(28×29) + (2×1)] / (30×30) = 814/900 = 0.904

**Cohen's Kappa (κ):**
κ = (Po - Pe) / (1 - Pe)
κ = (0.933 - 0.904) / (1 - 0.904)
κ = 0.029 / 0.096
κ = 0.302 (Fair agreement)

### Conflict Resolution
Two papers required third reviewer resolution:
1. **S019:** "Can consciousness be observed from large language model internal states?"
   - Resolution: Excluded (primary focus on consciousness, not ToM evaluation)
2. **S021:** "Spontaneous High-Order Generalization in Neural Theory-of-Mind Networks"
   - Resolution: Excluded (focuses on neural networks, not LLMs)

### Final Screening Results
- **Papers included for full-text review:** 28
- **Papers excluded:** 2
- **Exclusion reasons:**
  - Wrong focus (consciousness rather than ToM): 1
  - Wrong model type (neural networks, not LLMs): 1

## Quality Assessment Notes
While κ = 0.302 indicates fair agreement, the high observed agreement (93.3%) and clear criteria application suggest robust screening. The lower κ value is influenced by the high expected agreement given the focused search strategy that yielded highly relevant papers.
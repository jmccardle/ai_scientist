# Systematic Literature Review Search Strategy
## Theory of Mind Capabilities in Large Language Models

### Research Questions
1. What methods have been developed to evaluate Theory of Mind in LLMs?
2. What capabilities and limitations have been identified?
3. How do LLMs compare to human Theory of Mind performance?

### Search Databases and Date
- **Databases:** OpenAlex, arXiv, Semantic Scholar, PubMed
- **Search Date:** November 14, 2025
- **Date Range:** January 2018 - December 2025

### Search Terms
Primary search strings:
1. "theory of mind" AND "large language model"
2. "theory of mind" AND (GPT OR BERT OR "language model")
3. "mental state" AND reasoning AND LLM
4. "false belief" AND "language model"
5. "social reasoning" AND AI
6. ToM AND benchmark AND LLM
7. "mindreading" AND "artificial intelligence"

### Inclusion Criteria
- Papers evaluating Theory of Mind capabilities in LLMs
- Papers developing ToM benchmarks or evaluation methods for AI
- Papers comparing human and LLM ToM capabilities
- Published between January 2018 and December 2025
- Full text available
- Written in English

### Exclusion Criteria
- Papers focusing solely on human ToM without AI component
- Technical papers on LLM architecture without ToM evaluation
- Non-English language papers
- Conference abstracts without full paper
- Duplicate publications

### Data Extraction Schema
For each included study:
1. **Study Identification**
   - Authors, Year, Title, DOI

2. **Methodology**
   - ToM task types (false belief, visual perspective-taking, emotion recognition, etc.)
   - Models evaluated (GPT-3, GPT-4, BERT, LaMDA, Claude, etc.)
   - Evaluation methods (zero-shot, few-shot, fine-tuned)

3. **Results**
   - Performance metrics (accuracy, F1, comparison scores)
   - Human baseline comparisons
   - Specific capabilities demonstrated
   - Limitations identified

4. **Quality Assessment**
   - Study design quality
   - Benchmark validity
   - Statistical rigor
   - Reproducibility

### Screening Process
1. **Title/Abstract Screening**: Apply inclusion/exclusion criteria
2. **Full-text Screening**: Detailed assessment for relevance
3. **Inter-rater Reliability**: Simulated two-reviewer process with conflict resolution
4. **Data Extraction**: Systematic extraction using predefined schema

### Quality Assessment Protocol
- Methodological rigor (experimental design, controls)
- Validity of ToM tasks used
- Statistical analysis appropriateness
- Generalizability of findings
- Potential biases

### PRISMA 2020 Compliance
This review follows all 27 items of the PRISMA 2020 checklist.